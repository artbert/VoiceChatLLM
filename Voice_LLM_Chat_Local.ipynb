{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "826ccd9b",
   "metadata": {},
   "source": [
    "### Setup Instructions\n",
    "\n",
    "This notebook contains the code for a voice-enabled LLM chat application. To run this notebook, you need to set up the environment and download the necessary models: check the '*Local Setup Instructions*' section of the README.md file.\n",
    "**Important note: this notebook is intended to run only in local environment.**\n",
    "\n",
    "### Usage Instructions\n",
    "\n",
    "Once the setup is complete and the models are loaded, you can run the application cells to start the voice chat interface.\n",
    "\n",
    "#### Starting the Application\n",
    "\n",
    "Run the code cell directly following the \"Starting The Application\" markdown heading. This cell initializes and displays the chat interface.\n",
    "\n",
    "#### Interacting with the Application\n",
    "\n",
    "After running the start cell, a chat interface will appear.\n",
    "*   **Voice Input:** Look for a microphone button and click it to start speaking. Speak clearly and concisely. Click the STOP button to stop recording. Your spoken input will be transcribed into text.\n",
    "*   **Text Input:** You can also typically type your message into a text box provided in the interface and press Enter or click a send button.\n",
    "*   **Receiving Responses:** The application will process your input using the LLM. The response will be displayed as text in the chat interface. If the Text-to-Speech model is working correctly, you will also hear the response spoken aloud.\n",
    "\n",
    "#### Stopping the Application\n",
    "\n",
    "To gracefully stop the application and release resources, run the code cell directly following the \"Stopping The Application\" markdown heading."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926f0408-5947-4334-b8cb-6a4f18dcbda5",
   "metadata": {},
   "source": [
    "### Libraries Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d835900f-9a70-4859-bd24-980c53a2d26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "from IPython.display import Javascript, HTML, display\n",
    "from vosk import Model, KaldiRecognizer\n",
    "import threading\n",
    "import ipywidgets as widgets\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from piper.voice import PiperVoice\n",
    "from utils.voice_llm_chat import VoiceLLMChatBackend\n",
    "from utils.voice_llm_chat_frontend import VoiceLLMChatFrontend_Local"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e445cd4-66e8-412a-8981-bff686092081",
   "metadata": {},
   "source": [
    "### Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae08997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List of available Piper voices\n",
    "# !python -m piper.download_voices\n",
    "\n",
    "# Download a specific Piper voice\n",
    "!python -m piper.download_voices en_US-danny-low\n",
    "\n",
    "# Models are stored in the Open Neural Network Exchange (ONNX) format\n",
    "piper_voice_name = \"en_US-danny-low.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0bd935-59de-4761-962d-3a9e01c78ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 16000\n",
    "\n",
    "# Load the Piper voice model\n",
    "try:\n",
    "    voice_model = PiperVoice.load(piper_voice_name)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Piper voice model file not found. Please ensure '{piper_voice_name}' is in the correct directory.\", file=sys.stderr)\n",
    "    voice_model = None\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred while loading the Piper model: {e}\", file=sys.stderr)\n",
    "    voice_model = None\n",
    "\n",
    "\"\"\" Load the Vosk speech recognition model.\n",
    "Here we use a relatively small model. You can download a larger, much more accurate speech recognition model.\n",
    "\"\"\"\n",
    "try:\n",
    "    speech_model = Model(model_name=\"vosk-model-en-us-0.22-lgraph\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Vosk model: {e}. Please ensure the model is downloaded and accessible.\", file=sys.stderr)\n",
    "    speech_model = None\n",
    "    speech_recognizer = None\n",
    "\n",
    "if speech_model:\n",
    "    try:\n",
    "        speech_recognizer = KaldiRecognizer(speech_model, sample_rate)\n",
    "        speech_recognizer.SetWords(True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating Vosk recognizer: {e}\", file=sys.stderr)\n",
    "        speech_recognizer = None\n",
    "\n",
    "# Initialization of the LLM model and tokenizer\n",
    "llm_model_name = \"Gensyn/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    llm_model_name, pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "llm_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08a4a61",
   "metadata": {},
   "source": [
    "### Voice LLM Chat Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea1dfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the system message that best meets your needs.\n",
    "llm_model_system_message = \"You are a supportive voice assistant that replies with one or two brief sentences. Your replies should avoid any text formatting.\"\n",
    "\n",
    "# You can test various parameter configurations.\n",
    "llm_model_temperature = 0.1\n",
    "llm_model_max_tokens = 256\n",
    "llm_model_top_k = 100\n",
    "llm_model_top_p = 1.0\n",
    "app = VoiceLLMChatBackend(llm_model, tokenizer, voice_model, speech_recognizer)\n",
    "# Initialization of LLM model parameters.\n",
    "app.set_model_parameters(llm_model_temperature, llm_model_max_tokens, llm_model_top_k, llm_model_top_p, locale=\"en\")\n",
    "app.set_system_message(llm_model_system_message)\n",
    "# app.should_print_logs = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c4bb59",
   "metadata": {},
   "source": [
    "### Ipywidgets Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20534fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output widget to display messages and recognized text\n",
    "app_output_widget = widgets.Output()\n",
    "# Display object to update the HTML output area\n",
    "html_display_area = display(HTML(\"\"), display_id=True)\n",
    "\n",
    "requestDataContainer = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Request Data Container',\n",
    "    description='Request Data Container',\n",
    "    layout = widgets.Layout(display='none')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5301d522-e431-4c98-a70a-072871b1511e",
   "metadata": {},
   "source": [
    "### Button Click Handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd85f091-5c35-4ff0-8b6d-86e5ede9bae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_lock = threading.Lock()\n",
    "\n",
    "def start_new_chat_py(button):\n",
    "    \"\"\"Starts a new chat session.\"\"\"\n",
    "    app.start_new_chat()\n",
    "    with output_lock:\n",
    "        html_display_area.update(Javascript(f'newChatStarted();'))\n",
    "\n",
    "def send_prompt_py(button):\n",
    "    \"\"\"Sends a user prompt to the LLM.\"\"\"\n",
    "    prompt = requestDataContainer.value.strip()\n",
    "    app.send_prompt(prompt)\n",
    "    with output_lock:\n",
    "        html_display_area.update(Javascript(f'promptSent();'))\n",
    "\n",
    "def interrupt_response_py(button):\n",
    "    \"\"\"Interrupts the LLM's response.\"\"\"\n",
    "    app.interrupt_response()\n",
    "    while app.is_model_working:\n",
    "        time.sleep(0.1)\n",
    "    response = app.get_last_response()\n",
    "    # Call JavaScript function to update the interrupted message\n",
    "    with output_lock:\n",
    "        html_display_area.update(Javascript(f'responseInterrupted(`{response}`, `{str(app.get_context_load())}`);'))\n",
    "\n",
    "def transcribe_py(button):\n",
    "    \"\"\"Transcribes audio to text.\"\"\"\n",
    "    transcription = app.transcribe(requestDataContainer.value)\n",
    "    with output_lock:\n",
    "        html_display_area.update(Javascript(f'displayTranscription(`{transcription}`);'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdad1580",
   "metadata": {},
   "source": [
    "### Functions controlling the application \n",
    "\n",
    "These functions control the start and stop of the application and the thread function that refreshes the conversation window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cde8a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_event = threading.Event()\n",
    "\n",
    "def update_conversation(html_display_area, app):\n",
    "    \"\"\"\n",
    "    Continuously updates the conversation in the HTML output area.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        while not stop_event.is_set():\n",
    "            data = app.get_completed_data_chunk()\n",
    "            if data is not None:\n",
    "                display_sentence, encoded_audio = data\n",
    "                with output_lock:\n",
    "                    if encoded_audio != \"\":\n",
    "                        html_display_area.update(Javascript(f'appendAudio(`{encoded_audio}`);'))\n",
    "                    if display_sentence != \"\":\n",
    "                        html_display_area.update(Javascript(f'updateMessage(`{display_sentence}`);'))\n",
    "            else:\n",
    "                with output_lock:\n",
    "                    html_display_area.update(Javascript(f'assistantResponseFinished(`{str(app.get_context_load())}`);'))\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred in speech_recognition thread: {e}\", file=sys.stderr)\n",
    "        stop_event.set()\n",
    "\n",
    "def start_application():\n",
    "    app.start()\n",
    "    stop_event.clear()\n",
    "    transcribe_thread = threading.Thread(target=update_conversation, args=(html_display_area,app))\n",
    "    transcribe_thread.start()\n",
    "\n",
    "def stop_application():\n",
    "    app.stop()\n",
    "    stop_event.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87365aaa-b8ce-49bf-80c5-086c92c425d2",
   "metadata": {},
   "source": [
    "### Buttons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49160364-6874-4cfc-ad42-5f8dddaf965b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The buttons below will be activated from JavaScript code.\n",
    "new_chat_button = widgets.Button(description='New_Chat',layout=widgets.Layout(display='none'))\n",
    "send_button = widgets.Button(description='Send_Prompt', layout=widgets.Layout(display='none'))\n",
    "transcribe_button = widgets.Button(description='Transcribe_Audio', layout=widgets.Layout(display='none'))\n",
    "stop_reply_button = widgets.Button(description='Stop_Reply',layout=widgets.Layout(display='none'))\n",
    "\n",
    "# Arrange widgets in a layout\n",
    "control_panel = widgets.HBox([new_chat_button, send_button, transcribe_button, stop_reply_button])\n",
    "\n",
    "# Link button clicks to the respective functions\n",
    "new_chat_button.on_click(start_new_chat_py)\n",
    "send_button.on_click(send_prompt_py)\n",
    "transcribe_button.on_click(transcribe_py)\n",
    "stop_reply_button.on_click(interrupt_response_py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf0c79f",
   "metadata": {},
   "source": [
    "### Application Frontend\n",
    "\n",
    "User interface components and JavaScript functions that manage voice recording and communication with the Python backend in the Colab environment will be imported as a ready-made module. This is an HTML document with an embedded stylesheet and JavaScript script. Its content can be viewed using `print(llmChatFrontend)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ce9a0f-e8c8-4d9a-93d3-7daff23d2e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "voiceLLmFrontend = VoiceLLMChatFrontend_Local(\n",
    "    # Setting up our assistant's logo.\n",
    "    assistantAvatarSrc = \"https://qwenlm.github.io/img/logo.png\",\n",
    "    # For the user, let this be the Golab logo.\n",
    "    userAvatarSrc = \"https://colab.research.google.com/img/colab_favicon_256px.png\"\n",
    "    )\n",
    "\n",
    "# Static HTML document with the application's interface.\n",
    "llmChatFrontend = voiceLLmFrontend.getDocument()\n",
    "# # You can also preview the content of the document.\n",
    "# print(llmChatFrontend)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bab9d5a-26eb-4149-b6e6-89cb7b4d5cbc",
   "metadata": {},
   "source": [
    "### Launching the Application\n",
    "\n",
    "You need to allow the browser to use the microphone if you haven't done so yet. When running the application for the first time, it may be necessary to rerun the code below. The first transcription takes a little longer due to the initialization of the speech recognition model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b420b39d-b40b-42df-9daa-a2ff3aad7944",
   "metadata": {},
   "outputs": [],
   "source": [
    "if app.initialized:\n",
    "    # Display the buttons and the output area\n",
    "    app_output_widget.outputs = []\n",
    "    display(control_panel, requestDataContainer, app_output_widget)\n",
    "    start_application()\n",
    "    \n",
    "    # Add the HTML structure for the message display area\n",
    "    app_output_widget.append_display_data(HTML(llmChatFrontend))\n",
    "else:\n",
    "    print(\"initialization failed\")\n",
    "    # In case of problems, set the 'should_print_logs' flag to 'True', reload the application, and check the logs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6d888a",
   "metadata": {},
   "source": [
    "### Stopping the Application\n",
    "\n",
    "To properly stop the application and release resources, uncomment and run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46334d4-294a-4ca3-9cf2-54dccec2598f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_application()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44209066",
   "metadata": {},
   "source": [
    "The entire conversation in the form of an unformatted list of text messages can be exported by referencing the object below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e284f54b-76bc-49e5-a74e-61345d47be19",
   "metadata": {},
   "outputs": [],
   "source": [
    "app.chat_messages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (torch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
