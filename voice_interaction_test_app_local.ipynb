{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3pFT50HhxlU"
   },
   "source": [
    "# Microphone Test, Voice Recognition, and Voice Synthesis App\n",
    "\n",
    "This Jupyter notebook demonstrates a simple application for testing microphone input, performing voice recognition using Vosk, and synthesizing speech using Piper. It's a basic example of how to integrate these components for a real-time voice interaction application within a notebook environment.\n",
    "**Important note: this notebook is intended to run only in local environment.**\n",
    "\n",
    "## Setup\n",
    "\n",
    "To run this notebook, you need to set up the environment and download the necessary models: check the '*Local Setup Instructions*' section of the README.md file.\n",
    "\n",
    "## Running the Application\n",
    "\n",
    "1.  Execute all the code cells sequentially from top to bottom.\n",
    "2.  After running the last code cell, you will see a \"Start App\" button and an output area.\n",
    "3.  Click the \"Start App\" button to begin recording from your microphone.\n",
    "4.  Speak into your microphone. The recognized text will appear in the output area.\n",
    "5.  The application will also synthesize the recognized text using the Piper voice.\n",
    "6.  To stop the application, say the termination keyword (default is \"goodbye\") or click the \"Stop App\" button that appears after starting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Piper Voice Selection\n",
    "For the application's needs, we will download a voice named 'Amy'. You can choose another voice from many available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ghLHY7I7h_Fh"
   },
   "outputs": [],
   "source": [
    "# # List of available Piper voices\n",
    "# !python -m piper.download_voices\n",
    "\n",
    "# Download a specific Piper voice\n",
    "!python -m piper.download_voices en_US-amy-medium\n",
    "\n",
    "# Models are stored in the Open Neural Network Exchange (ONNX) format\n",
    "piper_voice_name = \"en_US-amy-medium.onnx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MIwaaxYGiDQB"
   },
   "source": [
    "### Libraries Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0aME29xIiGqw"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import sys\n",
    "from queue import Queue\n",
    "from threading import Thread, Event, Lock\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, Javascript\n",
    "import webrtcvad\n",
    "from vosk import Model, KaldiRecognizer\n",
    "from piper.voice import PiperVoice\n",
    "import sounddevice as sd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67R5H0gyiJ1Z"
   },
   "source": [
    "### Configuration Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1pxp9Qa_iNNR"
   },
   "outputs": [],
   "source": [
    "# Audio settings\n",
    "sample_rate = 16000\n",
    "audio_chunk_size = 320 # Size of audio chunks (20ms at 16kHz)\n",
    "channels = 1 # Number of audio channels (mono)\n",
    "consecutive_silent_seconds_limit = 1 # Number of silent seconds to detect end of speech\n",
    "\n",
    "# Initialize WebRTC VAD with aggressive mode\n",
    "vad = webrtcvad.Vad()\n",
    "vad.set_mode(3)\n",
    "\n",
    "# Padding to add to the beginning of the audio buffer\n",
    "audio_buffer_padding = b'\\x00\\x00' * int(sample_rate * audio_chunk_size / 1000)\n",
    "\n",
    "# Keyword to terminate the application\n",
    "termination_keyword = \"goodbye\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WnqNDIgciOab"
   },
   "source": [
    "### Queues and Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e8fZGP8miYEP"
   },
   "outputs": [],
   "source": [
    "recordings_queue = Queue()\n",
    "recognized_texts_queue = Queue()\n",
    "\n",
    "stop_event = Event()\n",
    "audio_playback_interrupt_event = Event()\n",
    "\n",
    "output_lock = Lock()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ncbyN6Mibbh"
   },
   "source": [
    "### Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wqUsDhXnie6x"
   },
   "outputs": [],
   "source": [
    "# Load the Piper voice model\n",
    "try:\n",
    "    voice_model = PiperVoice.load(piper_voice_name)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Piper voice model file not found. Please ensure '{piper_voice_name}' is in the correct directory.\", file=sys.stderr)\n",
    "    voice_model = None\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred while loading the Piper model: {e}\", file=sys.stderr)\n",
    "    voice_model = None\n",
    "\n",
    "\"\"\" Load the Vosk speech recognition model.\n",
    "Here we use a relatively small model. You can download a larger, much more accurate speech recognition model.\n",
    "\"\"\"\n",
    "try:\n",
    "    speech_model = Model(model_name=\"vosk-model-en-us-0.22-lgraph\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Vosk model: {e}. Please ensure the model is downloaded and accessible.\", file=sys.stderr)\n",
    "    speech_model = None\n",
    "    speech_recognizer = None\n",
    "\n",
    "if speech_model:\n",
    "    try:\n",
    "        speech_recognizer = KaldiRecognizer(speech_model, sample_rate)\n",
    "        speech_recognizer.SetWords(True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating Vosk recognizer: {e}\", file=sys.stderr)\n",
    "        speech_recognizer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUHUzO1Cij-X"
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jU_qa3Hein0i"
   },
   "outputs": [],
   "source": [
    "def callback(indata, frames, time, status):\n",
    "    \"\"\"Callback for sounddevice input stream.\"\"\"\n",
    "    if status:\n",
    "        print(status, file=sys.stderr)\n",
    "    try:\n",
    "        recordings_queue.put(indata[:,0].tobytes())\n",
    "    except Exception as e:\n",
    "        print(f\"Error putting data into recordings queue: {e}\", file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6jj7UQkAipL5"
   },
   "outputs": [],
   "source": [
    "def record_microphone():\n",
    "    \"\"\"Continuously records audio from the microphone and puts it into a queue.\n",
    "\n",
    "    Uses sounddevice to create an input stream from the default microphone.\n",
    "    The `callback` function is used to process each audio block. The recording\n",
    "    continues until the `stop_event` is set.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with sd.InputStream(samplerate=sample_rate,\n",
    "                                channels=channels, callback=callback, dtype='int16', blocksize=audio_chunk_size):\n",
    "            while not stop_event.is_set():\n",
    "                sd.sleep(100)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during microphone recording: {e}\", file=sys.stderr)\n",
    "        # Signal other threads to stop in case of a recording error\n",
    "        stop_event.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EXgFH163ir5i"
   },
   "outputs": [],
   "source": [
    "def speech_recognition(html_display_area):\n",
    "    \"\"\"Processes the audio buffer, performs VAD, recognizes speech, and triggers synthesis.\n",
    "\n",
    "    Reads audio data from the `recordings_queue` queue, uses WebRTC VAD to detect speech,\n",
    "    buffers speech segments, and uses Vosk to recognize the spoken text. Recognized\n",
    "    text is displayed in the provided output widget and put into another queue\n",
    "    for voice synthesis. It also checks for a termination keyword to stop the application.\n",
    "\n",
    "    Args:\n",
    "        html_display_area (display): The display object to update with recognized text.\n",
    "    \"\"\"\n",
    "    if not speech_recognizer:\n",
    "        print(\"Speech recognition not available due to model loading error.\", file=sys.stderr)\n",
    "        return\n",
    "\n",
    "    buffer = audio_buffer_padding\n",
    "    in_speech = False\n",
    "    silence_threshold = 0\n",
    "\n",
    "    try:\n",
    "        while not stop_event.is_set():\n",
    "            try:\n",
    "                frames = recordings_queue.get(timeout=0.1)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Use WebRTC VAD to check if the current frames contain speech\n",
    "                is_speech = vad.is_speech(frames, sample_rate=sample_rate)\n",
    "            except Exception as e:\n",
    "                print(f\"Error during VAD processing: {e}\", file=sys.stderr)\n",
    "                continue\n",
    "\n",
    "            if is_speech:\n",
    "                # If speech is detected\n",
    "                in_speech = True\n",
    "                buffer += frames\n",
    "                silence_threshold = 0\n",
    "\n",
    "            elif in_speech:\n",
    "                # Check if the silence duration has exceeded the limit\n",
    "                if silence_threshold < consecutive_silent_seconds_limit * (sample_rate / audio_chunk_size):\n",
    "                    silence_threshold += 1\n",
    "                    buffer += frames\n",
    "                else:\n",
    "                    # If silence limit reached, process the buffered speech\n",
    "                    try:\n",
    "                        speech_recognizer.AcceptWaveform(buffer)\n",
    "                        result = json.loads(speech_recognizer.Result())\n",
    "                        recognized_text = result['text']\n",
    "                        if recognized_text:\n",
    "                            recognized_text = f\"{recognized_text.capitalize()}.\"\n",
    "                            with output_lock:\n",
    "                                html_display_area.update(Javascript(f'addMessage(\"{recognized_text}\");'))\n",
    "                            recognized_texts_queue.put(recognized_text)\n",
    "                            audio_playback_interrupt_event.clear()\n",
    "                            \n",
    "                            # Check if the termination keyword is in the recognized text\n",
    "                            if termination_keyword in recognized_text.lower():\n",
    "                                message = f\"Termination keyword detected: '{termination_keyword}'. Stopping...\"\n",
    "                                with output_lock:\n",
    "                                    html_display_area.update(Javascript(f'addMessage(\"{message}\", true);'))\n",
    "                                    stop_application(stop_button)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error during speech recognition processing: {e}\", file=sys.stderr)\n",
    "\n",
    "                    in_speech = False\n",
    "                    silence_threshold = 0\n",
    "                    buffer = audio_buffer_padding\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred in speech_recognition thread: {e}\", file=sys.stderr)\n",
    "        stop_event.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2AUCbIFBiwdF"
   },
   "outputs": [],
   "source": [
    "def voice_synthesis():\n",
    "    \"\"\"Generates voice output based on recognized text and plays the audio.\n",
    "\n",
    "    Reads recognized text from the `recognized_texts_queue` queue, synthesizes\n",
    "    speech using the Piper voice model, and plays the resulting audio using\n",
    "    sounddevice. Audio playback is interrupted if the `audio_playback_interrupt_event`\n",
    "    is set (e.g., when the user starts speaking again).\n",
    "    \"\"\"\n",
    "    if not voice_model:\n",
    "        print(\"Voice synthesis not available due to model loading error.\", file=sys.stderr)\n",
    "        return\n",
    "\n",
    "    sd.default.samplerate = voice_model.config.sample_rate\n",
    "\n",
    "    # Add a small padding of silence between synthesized speech segments\n",
    "    voice_padding = b'\\x00\\x00' * int(voice_model.config.sample_rate * 0.1)\n",
    "\n",
    "    try:\n",
    "        while not stop_event.is_set():\n",
    "            if audio_playback_interrupt_event.is_set():\n",
    "                sd.stop()\n",
    "            try:\n",
    "                text = recognized_texts_queue.get(timeout=0.1)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            if text and not audio_playback_interrupt_event.is_set(): # Stop voice synthesis when user speaks\n",
    "                try:\n",
    "                    gen = voice_model.synthesize(text)\n",
    "                    audio_chunks_int16 = []\n",
    "                    for chunk in gen:\n",
    "                        if audio_playback_interrupt_event.is_set():\n",
    "                            # If interrupted, close the generator and stop playback\n",
    "                            gen.close()\n",
    "                            sd.stop()\n",
    "                            break\n",
    "                        audio_chunks_int16.append(chunk.audio_int16_array)\n",
    "                    \n",
    "                    if audio_chunks_int16:\n",
    "                        concatenated_audio = np.concatenate(audio_chunks_int16)\n",
    "                        sd.play(concatenated_audio)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error during voice synthesis or playback: {e}\", file=sys.stderr)\n",
    "        sd.stop()\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred in voice_synthesis thread: {e}\", file=sys.stderr)\n",
    "        stop_event.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFhzJ1aAi2Le"
   },
   "source": [
    "### Button Click Handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PZ_1QLeIi5pI"
   },
   "outputs": [],
   "source": [
    "def start_application(button):\n",
    "    \"\"\"Starts recording, recognition, and synthesis threads.\n",
    "    Updates the UI to indicate the application is starting and listening.\n",
    "    \"\"\"\n",
    "\n",
    "    stop_event.clear()\n",
    "    \n",
    "    button.layout.display = 'none'\n",
    "    stop_button.layout = widgets.Layout(display='')\n",
    "    listening_indicator.layout = widgets.Layout(display='')\n",
    "    html_display_area.update(Javascript(f'addMessage(\"Starting...\", true);'))\n",
    "    record_thread = Thread(target=record_microphone)\n",
    "    transcribe_thread = Thread(target=speech_recognition, args=(html_display_area,))\n",
    "    voice_synthesis_thread = Thread(target=voice_synthesis)\n",
    "\n",
    "    record_thread.start()\n",
    "    transcribe_thread.start()\n",
    "    voice_synthesis_thread.start()\n",
    "    html_display_area.update(Javascript(f'addMessage(\"Listening...\", true);'))\n",
    "\n",
    "\n",
    "def stop_application(button):\n",
    "    \"\"\"Stops application threads and updates UI.\n",
    "    Sets the stop event to signal threads to terminate and updates the UI\n",
    "    to indicate that the application has stopped.\n",
    "    \"\"\"\n",
    "    button.layout.display = 'none'\n",
    "    listening_indicator.layout = widgets.Layout(display='none')\n",
    "    stop_event.set()\n",
    "    audio_playback_interrupt_event.set()\n",
    "    html_display_area.update(Javascript(f'''\n",
    "    addMessage(\"Application stopped.\", true);\n",
    "    addMessage(\"Run the cell to start the app again.\", true);\n",
    "    '''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting The Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ICvovSCgi7on"
   },
   "outputs": [],
   "source": [
    "displayArea = \"\"\"<html>\n",
    "\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Repead after me test app</title>\n",
    "    <style>\n",
    "        body {\n",
    "            margin: 0;\n",
    "            padding: 20px;\n",
    "        }\n",
    "\n",
    "        #chatContainer {\n",
    "            margin: 20px auto;\n",
    "            background-color: white;\n",
    "            padding: 15px;\n",
    "            border-radius: 10px;\n",
    "            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);\n",
    "            overflow-y: auto;\n",
    "            height: 70vh;\n",
    "            display: flex;\n",
    "            flex-direction: column;\n",
    "        }\n",
    "\n",
    "        .message {\n",
    "            display: flex;\n",
    "            align-items: flex-start;\n",
    "            margin: 2px 0;\n",
    "            justify-content: flex-start;\n",
    "        }\n",
    "\n",
    "        .message-content {\n",
    "            display: flex;\n",
    "            flex-direction: column;\n",
    "            max-width: 70%;\n",
    "        }\n",
    "\n",
    "        .outgoing .message-content {\n",
    "            /* Align text to the right for outgoing */\n",
    "            align-items: flex-end;\n",
    "        }\n",
    "\n",
    "        .message .text {\n",
    "            padding: 5px;\n",
    "            border-radius: 5px;\n",
    "            /* Allow text to take full width of message-content */\n",
    "            max-width: 100%;\n",
    "            /* Break long words */\n",
    "            word-wrap: break-word;\n",
    "            /* Needed for the pseudo-elements */\n",
    "            position: relative;\n",
    "        }\n",
    "\n",
    "        .outgoing .text {\n",
    "            /* Blue for outgoing */\n",
    "            background-color: #007bff;\n",
    "            color: white;\n",
    "        }\n",
    "\n",
    "        .logging .text {\n",
    "            /* Orange for logging */\n",
    "            background-color: #ffb74d;\n",
    "            color: black;\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "\n",
    "<body>\n",
    "    <div id=\"chatContainer\">\n",
    "    </div>\n",
    "    <script>\n",
    "        function addMessage(content, system = false) {\n",
    "            console.log(\"message: \", content);\n",
    "\n",
    "            var chatContainer = document.getElementById(\"chatContainer\");\n",
    "            var messageDiv = document.createElement(\"DIV\");\n",
    "            var contentDiv = document.createElement(\"DIV\");\n",
    "            var textDiv = document.createElement(\"DIV\");\n",
    "\n",
    "            textDiv.className = \"text\";\n",
    "            textDiv.innerText = system ? \"System: \" : \"You said: \";\n",
    "            textDiv.innerText += content;\n",
    "\n",
    "            contentDiv.className = \"message-content\";\n",
    "            contentDiv.appendChild(textDiv);\n",
    "            messageDiv.appendChild(contentDiv);\n",
    "            chatContainer.appendChild(messageDiv);\n",
    "            chatContainer.scrollTop = chatContainer.scrollHeight;\n",
    "\n",
    "            messageDiv.className = system ? \"message logging\" : \"message outgoing\";\n",
    "\n",
    "        }\n",
    "    </script>\n",
    "</body>\n",
    "\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "start_button = widgets.Button(\n",
    "    description='Start App',\n",
    "    disabled=False,\n",
    "    button_style='success',\n",
    "    tooltip='Start App',\n",
    "    icon='microphone'\n",
    ")\n",
    "\n",
    "stop_button = widgets.Button(\n",
    "    description='Stop App',\n",
    "    disabled=False,\n",
    "    button_style='danger',\n",
    "    tooltip='Stop App',\n",
    "    icon='stop',\n",
    "    layout=widgets.Layout(display='none') # Initially hidden\n",
    ")\n",
    "\n",
    "listening_indicator = widgets.HTML(\n",
    "    value=\"<span style='margin-left: 10px; color: #dc3545; font-weight: bold;'>Listening...</span>\",\n",
    "    placeholder='App is running',\n",
    "    description='',\n",
    "    layout=widgets.Layout(display='none') # Initially hidden\n",
    ")\n",
    "\n",
    "control_panel = widgets.HBox([start_button, stop_button, listening_indicator])\n",
    "\n",
    "# Output widget to display messages and recognized text\n",
    "app_output_widget = widgets.Output()\n",
    "# Display object to update the HTML output area\n",
    "html_display_area = display(HTML(\"\"), display_id=True)\n",
    "\n",
    "# Link button clicks to the respective functions\n",
    "start_button.on_click(start_application)\n",
    "stop_button.on_click(stop_application)\n",
    "\n",
    "# Display the buttons and the output area\n",
    "display(control_panel, app_output_widget)\n",
    "# Add the HTML structure for the message display area\n",
    "app_output_widget.append_display_data(HTML(displayArea))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
