{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/artbert/VoiceChatLLM/blob/main/Voice_Chat_With_Bielik_Colab.ipynb)\n",
        "\n",
        "![Runtime: Colab](https://img.shields.io/badge/runtime-Colab-4285F4?style=for-the-badge&logo=googlecolab&logoColor=white)\n",
        "![GPU required](https://img.shields.io/badge/GPU-Required-red?style=for-the-badge&logo=nvidia&logoColor=white)"
      ],
      "metadata": {
        "id": "u1DbfpTp11_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div>\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/artbert/VoiceChatLLM/refs/heads/main/assets/voice_chat_with_bielik_colab_demo.png\" alt = \"Bielik Demo\" width=\"640\"/>\n",
        "</center>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "Sgx0VGfiJuNm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "518a38d1"
      },
      "source": [
        "## Instrukcja konfiguracji\n",
        "\n",
        "Zanim odpalimy nasz czat głosowy z LLM, szybko przygotuj środowisko – instalacja bibliotek i modeli zajmuje dosłownie chwilę. Pobieranie Bielika może jednak zająć trochę dłużej.\n",
        "\n",
        "### Wymagania wstępne\n",
        "\n",
        "- Środowisko Google Colab  \n",
        "- Python 3\n",
        "\n",
        "### Instalacja bibliotek\n",
        "\n",
        "Uruchom komórkę z `pip install`, by pobrać potrzebne pakiety: piper, ffmpeg, vosk, accelerate i bitsandbytes.\n",
        "\n",
        "### Pobieranie modeli\n",
        "\n",
        "Ta aplikacja opiera się na trzech modelach:  \n",
        "- Piper (Text-to-Speech)  \n",
        "- Vosk (`vosk-model-small-pl-0.22`) do rozpoznawania mowy  \n",
        "- Bielik (`speakleash/Bielik-11B-v2.6-Instruct`) jako nasz LLM  \n",
        "\n",
        "Vosk pobierze się automatycznie przy pierwszym wywołaniu `vosk.Model`. Bielik zostanie ściągnięty i załadowany przez `transformers`. Potrzebujesz karty graficznej, żeby LLM działał płynnie.\n",
        "\n",
        "### Uruchamianie aplikacji\n",
        "\n",
        "Gdy wszystkie biblioteki i modele są gotowe, odpal pozostałe komórki w notatniku krok po kroku, żeby uruchomić interfejs czatu i od razu go przetestować."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instrukcja użytkowania\n",
        "\n",
        "Tu zaczyna się zabawa – sprawdź, jak działa czat głosowy z LLM.\n",
        "\n",
        "### Uruchamianie aplikacji\n",
        "\n",
        "Znajdź komórkę zaraz po nagłówku „Uruchamianie aplikacji” i ją wykonaj. Interfejs czatu pojawi się od razu.\n",
        "\n",
        "### Interakcja z aplikacją\n",
        "\n",
        "Po starcie zobaczysz prosty panel czatu.\n",
        "\n",
        "#### Wprowadzanie wiadomości użytkownika\n",
        "\n",
        "Masz dwie opcje:\n",
        "\n",
        "- Wejście głosowe: kliknij ikonę mikrofonu, mów wyraźnie, kliknij STOP, by zakończyć nagranie. Transkrypcja pojawi się w polu tekstowym. Przed wysłaniem możesz wprowadzić korektę tekstu.\n",
        "- Wejście tekstowe: wpisz wiadomość i naciśnij Enter lub kliknij przycisk wysyłania.\n",
        "\n",
        "Odpowiedź od LLM pojawi się jako tekst, a jeśli TTS działa poprawnie, usłyszysz ją również na głos.\n",
        "\n",
        "#### Sterowanie czatem\n",
        "\n",
        "Asystent \"pamięta\" całość dotychczasowej konwersacji (dokładnie to, co widać w oknie) i jest to ograniczone przez szerokość okna kontekstowego wybranego modelu językowego.<br>Aby rozpocząć nowy czat, kliknij odpowiedni przycisk.\n",
        "\n",
        "W dowolnym momencie możesz również przerwać odpowiedź asystenta.\n",
        "\n",
        "\n",
        "### Zatrzymywanie aplikacji\n",
        "\n",
        "Gdy skończysz testować, uruchom komórkę pod nagłówkiem „Zatrzymywanie aplikacji”. To zwolni zasoby i poprawnie zakończy działanie czatu.\n",
        "\n",
        "---\n",
        "\n",
        "Nie krępuj się eksperymentować i zobaczyć, dokąd zaprowadzi Cię rozmowa z naszym Bielikiem!"
      ],
      "metadata": {
        "id": "Gjmm-1Zlwd8j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modele z kontrolą dostępu\n",
        "\n",
        "Na niektóre modele zostały nałożone zabezpieczenia w postaci kontrolowanego dostępu (gated models). Jeśli chciałbyś z tych modeli skorzystać, to należy podać token użytkownika, który można wygenerować na portalu HuggingFace: <br>\n",
        "- tokeny użytkownika:\n",
        "https://huggingface.co/settings/tokens\n",
        "- więcej informacji o tokenach:\n",
        "https://huggingface.co/docs/hub/security-tokens <br>\n",
        "\n",
        "Odkomentuj i uruchom poniższy kod, aby zainstalować \"huggingface_hub[cli]\" i zalogować się."
      ],
      "metadata": {
        "id": "F6rBBFo0ew_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -U \"huggingface_hub[cli]\" -q\n",
        "# !huggingface-cli login"
      ],
      "metadata": {
        "id": "gsu9uZWKewIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1985b34"
      },
      "source": [
        "### Instalacja bibliotek"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DX2hT68avaZt"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "!pip install accelerate -q\n",
        "!pip install -i https://pypi.org/simple/ bitsandbytes -q\n",
        "\n",
        "!pip3 install piper-tts -q\n",
        "!pip3 install ffmpeg-python -q\n",
        "!pip3 install vosk -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8a2a52d"
      },
      "source": [
        "### Pobieranie głosów Piper"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an application folder.\n",
        "!mkdir voice_llm_chat\n",
        "%cd voice_llm_chat\n",
        "\n",
        "# Pobierzmy jakieś fajne głosy.\n",
        "\n",
        "!wget -q -O pl_PL-mc_speech-medium.onnx https://huggingface.co/rhasspy/piper-voices/resolve/v1.0.0/pl/pl_PL/mc_speech/medium/pl_PL-mc_speech-medium.onnx?download=true\n",
        "!wget -q -O pl_PL-mc_speech-medium.onnx.json https://huggingface.co/rhasspy/piper-voices/resolve/v1.0.0/pl/pl_PL/mc_speech/medium/pl_PL-mc_speech-medium.onnx.json?download=true\n",
        "\n",
        "!wget -q -O pl_PL-darkman-medium.onnx https://huggingface.co/rhasspy/piper-voices/resolve/v1.0.0/pl/pl_PL/darkman/medium/pl_PL-darkman-medium.onnx?download=true\n",
        "!wget -q -O pl_PL-darkman-medium.onnx.json https://huggingface.co/rhasspy/piper-voices/resolve/v1.0.0/pl/pl_PL/darkman/medium/pl_PL-darkman-medium.onnx.json?download=true\n",
        "\n",
        "piper_voices = {\"mc_speech\": \"pl_PL-mc_speech-medium.onnx\", \"darkman\": \"pl_PL-darkman-medium.onnx\"}"
      ],
      "metadata": {
        "id": "MBk8ONG7xFD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31f6e96d"
      },
      "source": [
        "### Import bibliotek"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import sys\n",
        "import json\n",
        "from IPython.display import HTML, display\n",
        "from vosk import Model, KaldiRecognizer\n",
        "from base64 import b64decode\n",
        "from google.colab import output\n",
        "import ffmpeg\n",
        "import threading\n",
        "import ipywidgets as widgets\n",
        "import IPython\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "from piper.voice import PiperVoice"
      ],
      "metadata": {
        "id": "zrKCJhdmxIkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c05a3bfa"
      },
      "source": [
        "### Pobieranie skryptów czatu głosowego LLM\n",
        "\n",
        "Kod źródłowy programu można sprawdzić na GitHubie pod [tym](https://github.com/artbert/VoiceChatLLM) linkiem."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://raw.githubusercontent.com/artbert/VoiceChatLLM/refs/heads/main/utils/voice_llm_chat.py\n",
        "!wget -q https://raw.githubusercontent.com/artbert/VoiceChatLLM/refs/heads/main/utils/voice_llm_chat_frontend.py"
      ],
      "metadata": {
        "id": "NxKsz2X340KB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from voice_llm_chat import VoiceLLMChatBackend\n",
        "from voice_llm_chat_frontend import VoiceLLMChatFrontend_Colab"
      ],
      "metadata": {
        "id": "qIdIbqKwTv-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3216912a"
      },
      "source": [
        "### Zmienne konfiguracyjne"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Wybierz wiadomość systemową, która najlepiej pasuje do twoich potrzeb.\n",
        "llm_model_system_message = \"Jesteś pomocnym asystentem głosowym, który odpowiada w jednym lub dwóch krótkich zdaniach. Twoje odpowiedzi powinny unikać jakiegokolwiek formatowania tekstu.\"\n",
        "\n",
        "# Możesz przetestować różne konfiguracje parametrów.\n",
        "llm_model_temperature = 0.1\n",
        "llm_model_max_tokens = 2048\n",
        "llm_model_top_k = 100\n",
        "llm_model_top_p = 1"
      ],
      "metadata": {
        "id": "Va4QkXvMDV0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65caa1ba"
      },
      "source": [
        "### Ładowanie modeli"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Wybieramy jeden z pobranych głosów i ładujemy syntezator mowy.\n",
        "chosen_piper_voice = piper_voices[\"mc_speech\"]\n",
        "try:\n",
        "    voice_model = PiperVoice.load(chosen_piper_voice)\n",
        "except FileNotFoundError:\n",
        "    print(f\"\"\"Error: Piper voice model file not found. Please ensure '{chosen_piper_voice}' is in the correct directory.\"\"\", file=sys.stderr)\n",
        "    voice_model = None\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred while loading the Piper model: {e}\", file=sys.stderr)\n",
        "    voice_model = None\n",
        "\n",
        "\"\"\" Ładujemy Vosk speech recognition model.\n",
        "W przypadku języka polskiego mamy do dyspozycji tylko jeden mały model.\n",
        "\"\"\"\n",
        "sample_rate = 16000\n",
        "try:\n",
        "    speech_model = Model(model_name=\"vosk-model-small-pl-0.22\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading Vosk model: {e}. Please ensure the model is downloaded and accessible.\", file=sys.stderr)\n",
        "    speech_model = None\n",
        "    speech_recognizer = None\n",
        "\n",
        "if speech_model:\n",
        "    try:\n",
        "        speech_recognizer = KaldiRecognizer(speech_model, sample_rate)\n",
        "        speech_recognizer.SetWords(True)\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating Vosk recognizer: {e}\", file=sys.stderr)\n",
        "        speech_recognizer = None\n",
        "\n",
        "# Inicjalizacja modelu LLM i tokenizera.\n",
        "# Możesz wybrać inny model. Pamiętaj, że może on mieć inne wymagania systemowe.\n",
        "# llm_model_name = \"speakleash/Bielik-11B-v2.3-Instruct\"\n",
        "llm_model_name = \"speakleash/Bielik-11B-v2.6-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "# Za pomocą kwantyzacji ograniczymy znacznie użycie pamięci.\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16\n",
        "            )\n",
        "\n",
        "llm_model = AutoModelForCausalLM.from_pretrained(llm_model_name,\n",
        "                                             dtype=torch.bfloat16,\n",
        "                                             quantization_config=quantization_config,\n",
        "                                             pad_token_id = tokenizer.pad_token_id,\n",
        "                                             ).eval()"
      ],
      "metadata": {
        "id": "solqjiFSxUdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59523171"
      },
      "source": [
        "### Testowanie modeli\n",
        "\n",
        "Zaleca się przetestowanie mikrofonu, modułu rozpoznawania mowy i syntezatora mowy. Aby to zrobić, uruchom poniższy kod. Kliknij przycisk **Record**, powiedz coś po polsku, a następnie zakończ klikając przycisk **Stop**. Przy pierwszym uruchomieniu zezwól przeglądarce na dostęp do mikrofonu i ponownie uruchom kod."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Javascript, Audio, display\n",
        "from google.colab.output import eval_js\n",
        "\n",
        "# Prosty kod Javascript, który umożliwi nagrywanie sygnału z mikrofonu i zakodowanie go w formacie tekstowym ASCII.\n",
        "js = Javascript('''async function recordAudio() {\n",
        "    const div = document.createElement('div');\n",
        "    const startRecord = document.createElement('button');\n",
        "    const stopRecord = document.createElement('button');\n",
        "\n",
        "    startRecord.textContent = 'Record';\n",
        "    stopRecord.textContent = 'Stop';\n",
        "\n",
        "    document.body.appendChild(div);\n",
        "    div.appendChild(startRecord);\n",
        "\n",
        "    const stream = await navigator.mediaDevices.getUserMedia({audio:true});\n",
        "    let audioRecorder = new MediaRecorder(stream);\n",
        "\n",
        "    await new Promise((resolve) => startRecord.onclick = resolve);\n",
        "    startRecord.replaceWith(stopRecord);\n",
        "    audioRecorder.start();\n",
        "\n",
        "    await new Promise((resolve) => stopRecord.onclick = resolve);\n",
        "    audioRecorder.stop();\n",
        "    let recData = await new Promise((resolve) => audioRecorder.ondataavailable = resolve);\n",
        "    let arrBuff = await recData.data.arrayBuffer();\n",
        "    stream.getAudioTracks()[0].stop();\n",
        "    div.remove();\n",
        "\n",
        "    let binaryString = '';\n",
        "    let bytes = new Uint8Array(arrBuff);\n",
        "    bytes.forEach((byte) => { binaryString += String.fromCharCode(byte) });\n",
        "\n",
        "    const url = URL.createObjectURL(recData.data);\n",
        "    const player = document.createElement('audio');\n",
        "    player.controls = true;\n",
        "    player.src = url;\n",
        "    document.body.appendChild(player);\n",
        "\n",
        "    return btoa(binaryString);\n",
        "}\n",
        "''')\n",
        "\n",
        "# Dekodowanie formatu tekstowego do standardowego formatu binarnego.\n",
        "def get_audio(data):\n",
        "    if data is not None:\n",
        "        try:\n",
        "            binary = b64decode(data)\n",
        "        except:\n",
        "            print(\"Błąd podczas dekodowania. Sprawdź format danych wejściowych.\")\n",
        "        finally:\n",
        "            process = (ffmpeg\n",
        "            .input('pipe:0')\n",
        "            .output('-', format='s16le', acodec='pcm_s16le', ac=1, ar='16k')\n",
        "            .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True)\n",
        "            )\n",
        "            output, err = process.communicate(input=binary)\n",
        "            return output\n",
        "\n",
        "# Transkrypcja nagranego sygnału.\n",
        "def transcribe(data):\n",
        "    audio = get_audio(data)\n",
        "    if audio is not None:\n",
        "        speech_recognizer.AcceptWaveform(audio)\n",
        "        result = json.loads(speech_recognizer.FinalResult())\n",
        "        recognized_text = result['text']\n",
        "        if recognized_text:\n",
        "            recognized_text = recognized_text.capitalize() + \".\"\n",
        "\n",
        "        return recognized_text\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "display(js)\n",
        "try:\n",
        "    obj = eval_js('recordAudio({})')\n",
        "    transcription = transcribe(obj)\n",
        "    print(f\"Rozpoznany tekst: {transcription}\")\n",
        "\n",
        "    for audio_chunk in voice_model.synthesize(transcription):\n",
        "        display(Audio(audio_chunk.audio_int16_array, autoplay=True, rate=audio_chunk.sample_rate))\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Zezwól przeglądarce na dostęp do mikrofonu i uruchom ponownie kod.\")\n",
        "    print(f\"Błąd: {e}\")"
      ],
      "metadata": {
        "id": "J_djLZ25ZP7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07b35a7c"
      },
      "source": [
        "### Frontend aplikacji\n",
        "\n",
        "Komponenty interfejsu użytkownika i funkcje JavaScript, które zarządzają nagrywaniem głosu, i komunikacją z backendem Pythona w środowisku Colab zaimportujemy jako gotowiec. Jest to dokument HTML z osadzonym arkuszem stylów i skryptem Javascript. Jego zawartość można podejrzeć przez `print(llmChatFrontend)` lub bezpośrednio na GitHubie (link wyżej)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "voiceLLmFrontend = VoiceLLMChatFrontend_Colab(\n",
        "    # Ustawiamy logo naszego asystenta...\n",
        "    assistantAvatarSrc = \"https://bielik.ai/wp-content/uploads/2024/08/Bielik_Secondary_Rabarbar-300x82.webp\",\n",
        "    # Dla użytkownika niech to będzie logo Golab\n",
        "    userAvatarSrc = \"https://colab.research.google.com/img/colab_favicon_256px.png\"\n",
        "    )\n",
        "\n",
        "llmChatFrontend = voiceLLmFrontend.getDocument()\n",
        "# # Można również podejrzeć zawartość dokumentu\n",
        "# print(llmChatFrontend)"
      ],
      "metadata": {
        "id": "7MmV8aJkEHCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pobrana wersja frontendu będzie korzystać z pięciu funkcji wywoływanych z poziomu kodu Javascript. Poniżej znajduje się klasa z implementacją tych metod."
      ],
      "metadata": {
        "id": "RD5bYLFVSNs2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "694ede38"
      },
      "source": [
        "### Klasa naszego programu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LLMChatApp:\n",
        "    \"\"\"Main application class for the voice-enabled LLM chat.\"\"\"\n",
        "    def __init__(self, llm_model, tokenizer, voice_model, speech_recognizer):\n",
        "        \"\"\"Initializes the LLMChatApp with required models and components.\"\"\"\n",
        "        self.output_lock = threading.Lock()\n",
        "        # Output widget to display messages and recognized text\n",
        "        self.app_output_widget = widgets.Output()\n",
        "\n",
        "        # Preparing application's backend\n",
        "        self.app = VoiceLLMChatBackend(llm_model, tokenizer, voice_model, speech_recognizer)\n",
        "        # Passing parameters to the model.\n",
        "        self.app.set_model_parameters(llm_model_temperature, llm_model_max_tokens, llm_model_top_k, llm_model_top_p, locale=\"pl\")\n",
        "        self.app.set_system_message(llm_model_system_message)\n",
        "\n",
        "        # # If the application does not appear to function as intended, enable this flag.\n",
        "        # self.app.should_print_logs = True\n",
        "\n",
        "        self.initialized = self.app.initialized\n",
        "\n",
        "    def new_chat(self):\n",
        "        \"\"\"Starts a new chat session.\"\"\"\n",
        "        self.app.start_new_chat()\n",
        "        return IPython.display.JSON({\"response\": \"new chat created\"})\n",
        "\n",
        "    def send_prompt(self, prompt):\n",
        "        \"\"\"Sends a user prompt to the LLM.\"\"\"\n",
        "        self.app.send_prompt(prompt)\n",
        "        return IPython.display.JSON({\"response\": \"New prompt sent\"})\n",
        "\n",
        "    def fetch_data(self):\n",
        "        \"\"\"Fetches completed data chunks from the LLM chat backend.\"\"\"\n",
        "        try:\n",
        "            data = self.app.get_completed_data_chunk()\n",
        "            if data is not None:\n",
        "                display_sentence, encoded_audio = data\n",
        "                \"\"\"Many models return text formatted using Markdown notation, mainly headers and bolding.\n",
        "                Here they need to be removed because they are not recognized by the downloaded version of the frontend.\"\"\"\n",
        "                display_sentence = display_sentence.replace(\"*\", \"\").replace(\"_\", \"\").replace(\"#\", \"\")\n",
        "                result = {\n",
        "                    \"resp\": display_sentence,\n",
        "                    \"finish\": \"false\"\n",
        "                }\n",
        "                if encoded_audio != \"\":\n",
        "                    result[\"audio\"] = encoded_audio\n",
        "\n",
        "                return IPython.display.JSON(\n",
        "                                result\n",
        "                        )\n",
        "            else:\n",
        "                return IPython.display.JSON(\n",
        "                {\"resp\": \"\", \"finish\": \"true\", \"context\": str(self.app.get_context_load())}\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Error in  fetch_data: {e}\")\n",
        "\n",
        "    def interrupt_response(self):\n",
        "        \"\"\"Interrupts the LLM's response generation.\"\"\"\n",
        "        self.app.interrupt_response()\n",
        "        while self.app.is_model_working:\n",
        "            time.sleep(0.1)\n",
        "        response = self.app.get_last_response()\n",
        "        return IPython.display.JSON(\n",
        "            {\n",
        "                \"resp\": response,\n",
        "                \"finish\": \"true\",\n",
        "                \"context\": str(self.app.get_context_load()),\n",
        "            })\n",
        "\n",
        "    def transcribe(self, data):\n",
        "        \"\"\"Transcribes audio data using the speech recognizer.\"\"\"\n",
        "        transcription = self.app.transcribe(data)\n",
        "        return IPython.display.JSON({\"result\": transcription})\n",
        "\n",
        "    def start_application(self):\n",
        "        \"\"\"Starts the main application logic.\"\"\"\n",
        "        self.app.start()\n",
        "\n",
        "    def stop_application(self):\n",
        "        \"\"\"Stops the main application logic.\"\"\"\n",
        "        self.app.stop()\n",
        "\n",
        "    def register_callbacks(self):\n",
        "        \"\"\"Registers the class methods as Colab output callbacks.\"\"\"\n",
        "        output.register_callback(\"notebook.new_chat\", self.new_chat)\n",
        "        output.register_callback(\"notebook.fetch_data\", self.fetch_data)\n",
        "        output.register_callback(\"notebook.transcribe\", self.transcribe)\n",
        "        output.register_callback(\"notebook.interrupt_response\", self.interrupt_response)\n",
        "        output.register_callback(\"notebook.send_prompt\", self.send_prompt)\n"
      ],
      "metadata": {
        "id": "o2yclXtJy2H4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fca4cf2"
      },
      "source": [
        "### Inicjalizacja aplikacji"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "app_instance = LLMChatApp(llm_model, tokenizer, voice_model, speech_recognizer)\n",
        "\n",
        "app_instance.register_callbacks()"
      ],
      "metadata": {
        "id": "Zb84D562xdd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "873dbcb4"
      },
      "source": [
        "### Uruchamianie aplikacji\n",
        "\n",
        "Musisz zezwolić przeglądarce na korzystanie z mikrofonu, jeśli nie zrobiłeś tego jeszcze podczas testów. Przy pierwszym uruchomieniu aplikacji może być konieczne ponowne uruchomienie poniższego kodu. Pierwsza transkrypcja trwa nieco dłużej ze względu na inicjalizację modelu rozpoznawania mowy."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if app_instance.initialized:\n",
        "    app_instance.app_output_widget.outputs = []\n",
        "    display(app_instance.app_output_widget)\n",
        "\n",
        "    app_instance.start_application()\n",
        "\n",
        "    app_instance.app_output_widget.append_display_data(HTML(llmChatFrontend))\n",
        "else:\n",
        "    print(\"initialization failed\")\n",
        "    # W razie problemów, ustaw flagę 'should_print_logs' na 'True', załaduj aplikację ponownie i sprawdź logi."
      ],
      "metadata": {
        "id": "XaytpftQxktk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b72c007e"
      },
      "source": [
        "### Zatrzymywanie aplikacji\n",
        "\n",
        "Aby prawidłowo zatrzymać aplikację i zwolnić zasoby, odkomentuj i uruchom poniższy kod."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# app_instance.stop_application()"
      ],
      "metadata": {
        "id": "p7-OR4umx3gL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Całość konwersacji w postaci niesformatowanej listy wiadomości tekstowych można wyeksportować przez odwołanie do poniższego obiektu:"
      ],
      "metadata": {
        "id": "NTQPndvbSyld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# app_instance.app.chat_messages"
      ],
      "metadata": {
        "id": "t5kOpesAUBop"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}