{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/artbert/VoiceChatLLM/blob/main/Voice_LLM_Chat_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "nsp1ml6GQcd9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Instructions\n",
        "\n",
        "This notebook contains the code for a voice-enabled LLM chat application. To run this application, you need to set up your environment by installing the necessary libraries and downloading the required models.\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "*   **Google Colab Environment:** This notebook is designed to run in a Google Colab environment.\n",
        "*   **Python 3:** Ensure you have Python 3 installed.\n",
        "\n",
        "### Library Installation\n",
        "\n",
        "The required Python libraries are installed using `pip`. The following cell in the notebook handles these installations:\n",
        "\n",
        "```python\n",
        "!pip3 install piper-tts -q\n",
        "!pip3 install ffmpeg-python -q\n",
        "!pip3 install vosk -q\n",
        "```\n",
        "\n",
        "Simply run this cell to install the necessary packages.\n",
        "\n",
        "### Model Downloads\n",
        "\n",
        "This application requires a Piper voice model for Text-to-Speech (TTS) and a Vosk speech recognition model for Speech-to-Text (STT), as well as a Large Language Model (LLM).\n",
        "\n",
        "The Vosk speech recognition model (`vosk-model-en-us-0.22-lgraph`) is automatically downloaded by the `vosk.Model` constructor if not already present.\n",
        "\n",
        "The LLM model (`Gensyn/Qwen2.5-0.5B-Instruct`) is automatically downloaded and loaded using the `transformers` library. This is a very lightweight LLM language model that **DOES NOT** require a GPU environment.\n",
        "\n",
        "Run the respective code cells in the notebook to download and load these models.\n",
        "\n",
        "### Running the Application\n",
        "\n",
        "Once the libraries are installed and models are downloaded, you can run the remaining code cells in the notebook sequentially to start and interact with the voice chat application."
      ],
      "metadata": {
        "id": "T7G2gGZ1vyHI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usage Instructions\n",
        "\n",
        "Once the setup is complete and the models are loaded, you can run the application cells to start the voice chat interface.\n",
        "\n",
        "### Starting the Application\n",
        "\n",
        "Run the code cell directly following the \"Starting The Application\" markdown heading. This cell initializes and displays the chat interface.\n",
        "\n",
        "### Interacting with the Application\n",
        "\n",
        "After running the start cell, a chat interface will appear.\n",
        "*   **Voice Input:** Look for a microphone button and click it to start speaking. Speak clearly and concisely. Click the STOP button to stop recording. Your spoken input will be transcribed into text.\n",
        "*   **Text Input:** You can also typically type your message into a text box provided in the interface and press Enter or click a send button.\n",
        "*   **Receiving Responses:** The application will process your input using the LLM. The response will be displayed as text in the chat interface. If the Text-to-Speech model is working correctly, you will also hear the response spoken aloud.\n",
        "\n",
        "### Stopping the Application\n",
        "\n",
        "To gracefully stop the application and release resources, run the code cell directly following the \"Stopping The Application\" markdown heading."
      ],
      "metadata": {
        "id": "Gjmm-1Zlwd8j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Libraries Installation"
      ],
      "metadata": {
        "id": "oXUqp3fbw_Ob"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DX2hT68avaZt"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "!pip3 install piper-tts -q\n",
        "!pip3 install ffmpeg-python -q\n",
        "!pip3 install vosk -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Piper Voices Download"
      ],
      "metadata": {
        "id": "RNDIdC62UERS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an application folder.\n",
        "!mkdir voice_llm_chat\n",
        "%cd voice_llm_chat\n",
        "\n",
        "# Let's download some nice Piper voices\n",
        "\n",
        "!wget -q -O en_US-danny-low.onnx https://huggingface.co/rhasspy/piper-voices/resolve/v1.0.0/en/en_US/danny/low/en_US-danny-low.onnx?download=true\n",
        "!wget -q -O en_US-danny-low.onnx.json https://huggingface.co/rhasspy/piper-voices/resolve/v1.0.0/en/en_US/danny/low/en_US-danny-low.onnx.json?download=true\n",
        "\n",
        "!wget -q -O en_US-amy-medium.onnx https://huggingface.co/rhasspy/piper-voices/resolve/main/en/en_US/amy/medium/en_US-amy-medium.onnx?download=true\n",
        "!wget -q -O en_US-amy-medium.onnx.json https://huggingface.co/rhasspy/piper-voices/resolve/main/en/en_US/amy/medium/en_US-amy-medium.onnx.json?download=true\n",
        "\n",
        "!wget -q -O en_US-hfc_male-medium.onnx https://huggingface.co/rhasspy/piper-voices/resolve/main/en/en_US/hfc_male/medium/en_US-hfc_male-medium.onnx?download=true\n",
        "!wget -q -O en_US-hfc_male-medium.onnx.json https://huggingface.co/rhasspy/piper-voices/resolve/main/en/en_US/hfc_male/medium/en_US-hfc_male-medium.onnx.json?download=true\n",
        "\n",
        "!wget -q -O en_US-lessac-medium.onnx https://huggingface.co/rhasspy/piper-voices/resolve/main/en/en_US/lessac/medium/en_US-lessac-medium.onnx?download=true\n",
        "!wget -q -O en_US-lessac-medium.onnx.json https://huggingface.co/rhasspy/piper-voices/resolve/main/en/en_US/lessac/medium/en_US-lessac-medium.onnx.json?download=true\n",
        "\n",
        "!wget -q -O en_US-ryan-medium.onnx https://huggingface.co/rhasspy/piper-voices/resolve/main/en/en_US/ryan/medium/en_US-ryan-medium.onnx?download=true\n",
        "!wget -q -O en_US-ryan-medium.onnx.json https://huggingface.co/rhasspy/piper-voices/resolve/main/en/en_US/ryan/medium/en_US-ryan-medium.onnx.json?download=true\n",
        "\n",
        "\n",
        "piper_voices = {\"amy\": \"en_US-amy-medium.onnx\", \"danny\": \"en_US-danny-low.onnx\", \"hfc_male\": \"en_US-hfc_male-medium.onnx\", \"lessac\": \"en_US-lessac-medium.onnx\", \"ryan\": \"en_US-ryan-medium.onnx\"}\n"
      ],
      "metadata": {
        "id": "MBk8ONG7xFD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Libraries Import"
      ],
      "metadata": {
        "id": "JzzkT1FGACIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import sys\n",
        "import json\n",
        "from IPython.display import HTML, display\n",
        "from vosk import Model, KaldiRecognizer\n",
        "from base64 import b64decode\n",
        "from google.colab import output\n",
        "import ffmpeg\n",
        "import threading\n",
        "import ipywidgets as widgets\n",
        "import IPython\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "from piper.voice import PiperVoice"
      ],
      "metadata": {
        "id": "zrKCJhdmxIkh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download Voice LLM Chat Scripts\n",
        "\n",
        "You can check the source code of the program on GitHub at [this](https://github.com/artbert/VoiceChatLLM) link."
      ],
      "metadata": {
        "id": "mWMpKFPd4rPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://raw.githubusercontent.com/artbert/VoiceChatLLM/refs/heads/main/utils/voice_llm_chat.py\n",
        "!wget -q https://raw.githubusercontent.com/artbert/VoiceChatLLM/refs/heads/main/utils/voice_llm_chat_frontend.py"
      ],
      "metadata": {
        "id": "NxKsz2X340KB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from voice_llm_chat import VoiceLLMChatBackend\n",
        "from voice_llm_chat_frontend import VoiceLLMChatFrontend_Colab"
      ],
      "metadata": {
        "id": "qIdIbqKwTv-V"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configuration Variables"
      ],
      "metadata": {
        "id": "ntVBOJRrC4-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose the system message that best meets your needs.\n",
        "llm_model_system_message = \"You are a supportive voice assistant that replies with one or two brief sentences. Your replies should avoid any text formatting.\"\n",
        "\n",
        "llm_model_temperature = 0.1\n",
        "llm_model_max_tokens = 256\n",
        "llm_model_top_k = 100\n",
        "llm_model_top_p = 1"
      ],
      "metadata": {
        "id": "Va4QkXvMDV0i"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Models"
      ],
      "metadata": {
        "id": "fieFLS9fxTSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Piper voice model\n",
        "chosen_piper_voice = piper_voices[\"lessac\"]\n",
        "try:\n",
        "    voice_model = PiperVoice.load(chosen_piper_voice)\n",
        "except FileNotFoundError:\n",
        "    print(f\"\"\"Error: Piper voice model file not found. Please ensure '{chosen_piper_voice}' is in the correct directory.\"\"\", file=sys.stderr)\n",
        "    voice_model = None\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred while loading the Piper model: {e}\", file=sys.stderr)\n",
        "    voice_model = None\n",
        "\n",
        "\"\"\" Load the Vosk speech recognition model.\n",
        "Here we use a relatively small model. You can download a larger, much more accurate speech recognition model.\n",
        "\"\"\"\n",
        "sample_rate = 16000\n",
        "try:\n",
        "    speech_model = Model(model_name=\"vosk-model-en-us-0.22-lgraph\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading Vosk model: {e}. Please ensure the model is downloaded and accessible.\", file=sys.stderr)\n",
        "    speech_model = None\n",
        "    speech_recognizer = None\n",
        "\n",
        "if speech_model:\n",
        "    try:\n",
        "        speech_recognizer = KaldiRecognizer(speech_model, sample_rate)\n",
        "        speech_recognizer.SetWords(True)\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating Vosk recognizer: {e}\", file=sys.stderr)\n",
        "        speech_recognizer = None\n",
        "\n",
        "# Initialization of the LLM model and tokenizer.\n",
        "# You can choose any language model\n",
        "llm_model_name = \"Gensyn/Qwen2.5-0.5B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
        "llm_model = AutoModelForCausalLM.from_pretrained(\n",
        "    llm_model_name, pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "llm_model.eval()"
      ],
      "metadata": {
        "id": "solqjiFSxUdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing Models\n",
        "\n",
        "It is recommended to test the microphone, speech recognition module, and speech synthesizer. To do this, run the code below. Click **Start Recording** button, speak, then finish by clicking **Stop Recording**. When running for the first time, allow the browser access to the microphone and run the code again."
      ],
      "metadata": {
        "id": "xN-5Ka3dUAL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Javascript, Audio, display\n",
        "from google.colab.output import eval_js\n",
        "\n",
        "# A simple Javascript code that will allow recording a signal from the microphone and encoding it into a text format.\n",
        "js = Javascript('''async function recordAudio() {\n",
        "    const div = document.createElement('div');\n",
        "    const strtButton = document.createElement('button');\n",
        "    const stopButton = document.createElement('button');\n",
        "\n",
        "    strtButton.textContent = 'Start Recording';\n",
        "    stopButton.textContent = 'Stop Recording';\n",
        "\n",
        "    document.body.appendChild(div);\n",
        "    div.appendChild(strtButton);\n",
        "\n",
        "    const stream = await navigator.mediaDevices.getUserMedia({audio:true});\n",
        "    let recorder = new MediaRecorder(stream);\n",
        "\n",
        "    await new Promise((resolve) => strtButton.onclick = resolve);\n",
        "    strtButton.replaceWith(stopButton);\n",
        "    recorder.start();\n",
        "\n",
        "    await new Promise((resolve) => stopButton.onclick = resolve);\n",
        "    recorder.stop();\n",
        "    let recData = await new Promise((resolve) => recorder.ondataavailable = resolve);\n",
        "    let arrBuff = await recData.data.arrayBuffer();\n",
        "    stream.getAudioTracks()[0].stop();\n",
        "    div.remove();\n",
        "\n",
        "    let binaryString = '';\n",
        "    let bytes = new Uint8Array(arrBuff);\n",
        "    bytes.forEach((byte) => { binaryString += String.fromCharCode(byte) });\n",
        "\n",
        "    const url = URL.createObjectURL(recData.data);\n",
        "    const player = document.createElement('audio');\n",
        "    player.controls = true;\n",
        "    player.src = url;\n",
        "    document.body.appendChild(player);\n",
        "\n",
        "    return btoa(binaryString);\n",
        "}\n",
        "''')\n",
        "\n",
        "# Decoding the text format into a standard wave binary format.\n",
        "def get_audio(data):\n",
        "    if data is not None:\n",
        "        try:\n",
        "            binary = b64decode(data)\n",
        "        except:\n",
        "            print(\"Probably microphone is not allowed.\")\n",
        "        finally:\n",
        "            process = (ffmpeg\n",
        "            .input('pipe:0')\n",
        "            .output('-', format='s16le', acodec='pcm_s16le', ac=1, ar='16k')\n",
        "            .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True)\n",
        "            )\n",
        "            output, err = process.communicate(input=binary)\n",
        "            return output\n",
        "\n",
        "# Converting spoken audio into written text.\n",
        "def transcribe(data):\n",
        "    audio = get_audio(data)\n",
        "    if audio is not None:\n",
        "        speech_recognizer.AcceptWaveform(audio)\n",
        "        result = json.loads(speech_recognizer.FinalResult())\n",
        "        recognized_text = result['text']\n",
        "        if recognized_text:\n",
        "            recognized_text = recognized_text.capitalize() + \".\"\n",
        "\n",
        "        return recognized_text\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "display(js)\n",
        "obj = eval_js('recordAudio({})')\n",
        "transcription = transcribe(obj)\n",
        "\n",
        "print(f\"Recognized speech: {transcription}\")\n",
        "\n",
        "for audio_chunk in voice_model.synthesize(transcription):\n",
        "    display(Audio(audio_chunk.audio_int16_array, autoplay=True, rate=audio_chunk.sample_rate))"
      ],
      "metadata": {
        "id": "J_djLZ25ZP7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chat Application"
      ],
      "metadata": {
        "id": "t_ReNC_iy3EL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LLMChatApp:\n",
        "    \"\"\"Main application class for the voice-enabled LLM chat.\"\"\"\n",
        "    def __init__(self, llm_model, tokenizer, voice_model, speech_recognizer):\n",
        "        \"\"\"Initializes the LLMChatApp with required models and components.\"\"\"\n",
        "        self.output_lock = threading.Lock()\n",
        "        # Output widget to display messages and recognized text\n",
        "        self.app_output_widget = widgets.Output()\n",
        "\n",
        "        # Preparing application's backend\n",
        "        self.app = VoiceLLMChatBackend(llm_model, tokenizer, voice_model, speech_recognizer)\n",
        "        # Initialization of LLM model parameters.\n",
        "        self.app.set_model_parameters(llm_model_temperature, llm_model_max_tokens, llm_model_top_k, llm_model_top_p, locale=\"en\")\n",
        "        self.app.set_system_message(llm_model_system_message)\n",
        "\n",
        "        # # If the application does not appear to function as intended, enable this flag.\n",
        "        # self.app.should_print_logs = True\n",
        "\n",
        "        self.initialized = self.app.initialized\n",
        "\n",
        "    def new_chat(self):\n",
        "        \"\"\"Starts a new chat session.\"\"\"\n",
        "        self.app.start_new_chat()\n",
        "        return IPython.display.JSON({\"response\": \"new chat created\"})\n",
        "\n",
        "    def send_prompt(self, prompt):\n",
        "        \"\"\"Sends a user prompt to the LLM.\"\"\"\n",
        "        self.app.send_prompt(prompt)\n",
        "        return IPython.display.JSON({\"response\": \"New prompt sent\"})\n",
        "\n",
        "    def fetch_data(self):\n",
        "        \"\"\"Fetches completed data chunks from the LLM chat backend.\"\"\"\n",
        "        try:\n",
        "            data = self.app.get_completed_data_chunk()\n",
        "            if data is not None:\n",
        "                display_sentence, encoded_audio = data\n",
        "                result = {\n",
        "                    \"resp\": display_sentence,\n",
        "                    \"finish\": \"false\"\n",
        "                }\n",
        "                if encoded_audio != \"\":\n",
        "                    result[\"audio\"] = encoded_audio\n",
        "\n",
        "                return IPython.display.JSON(\n",
        "                                result\n",
        "                        )\n",
        "            else:\n",
        "                return IPython.display.JSON(\n",
        "                {\"resp\": \"\", \"finish\": \"true\", \"context\": str(self.app.get_context_load())}\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Error in  fetch_data: {e}\")\n",
        "\n",
        "    def interrupt_response(self):\n",
        "        \"\"\"Interrupts the LLM's response generation.\"\"\"\n",
        "        self.app.interrupt_response()\n",
        "        while self.app.is_model_working:\n",
        "            time.sleep(0.1)\n",
        "        response = self.app.get_last_response()\n",
        "        return IPython.display.JSON(\n",
        "            {\n",
        "                \"resp\": response,\n",
        "                \"finish\": \"true\",\n",
        "                \"context\": str(self.app.get_context_load()),\n",
        "            })\n",
        "\n",
        "    def transcribe(self, data):\n",
        "        \"\"\"Transcribes audio data using the speech recognizer.\"\"\"\n",
        "        transcription = self.app.transcribe(data)\n",
        "        return IPython.display.JSON({\"result\": transcription})\n",
        "\n",
        "    def start_application(self):\n",
        "        \"\"\"Starts the main application logic.\"\"\"\n",
        "        self.app.start()\n",
        "\n",
        "    def stop_application(self):\n",
        "        \"\"\"Stops the main application logic.\"\"\"\n",
        "        self.app.stop()\n",
        "\n",
        "    def register_callbacks(self):\n",
        "        \"\"\"Registers the class methods as Colab output callbacks.\"\"\"\n",
        "        output.register_callback(\"notebook.new_chat\", self.new_chat)\n",
        "        output.register_callback(\"notebook.fetch_data\", self.fetch_data)\n",
        "        output.register_callback(\"notebook.transcribe\", self.transcribe)\n",
        "        output.register_callback(\"notebook.interrupt_response\", self.interrupt_response)\n",
        "        output.register_callback(\"notebook.send_prompt\", self.send_prompt)\n"
      ],
      "metadata": {
        "id": "o2yclXtJy2H4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chat Application Frontend\n",
        "\n",
        "The user interface components of the application and JavaScript functions that manage voice recording and communication with the Python backend in Colab Environment."
      ],
      "metadata": {
        "id": "lJg58WwNGmUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "voiceLLmFrontend = VoiceLLMChatFrontend_Colab(\n",
        "    assistantAvatarSrc = \"https://qwenlm.github.io/img/logo.png\",\n",
        "    userAvatarSrc = \"https://colab.research.google.com/img/colab_favicon_256px.png\"\n",
        "    )\n",
        "\n",
        "# Static HTML document generating the application interface.\n",
        "llmChatFrontend = voiceLLmFrontend.getDocument()"
      ],
      "metadata": {
        "id": "7MmV8aJkEHCj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Application Initialization"
      ],
      "metadata": {
        "id": "ISyVazfNxlbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the new class\n",
        "app_instance = LLMChatApp(llm_model, tokenizer, voice_model, speech_recognizer)\n",
        "\n",
        "# A static HTML document that will allow for generating the application's interface.\n",
        "app_instance.register_callbacks()"
      ],
      "metadata": {
        "id": "Zb84D562xdd5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Starting The Application\n",
        "\n",
        "You need to allow your browser to use your microphone. When you first launch the application, it may be necessary to restart the code below. The first transcription takes a little longer due to the initialization of the speech recognition model."
      ],
      "metadata": {
        "id": "7_XKk1j8xt6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if app_instance.initialized:\n",
        "    app_instance.app_output_widget.outputs = []\n",
        "    display(app_instance.app_output_widget)\n",
        "\n",
        "    app_instance.start_application()\n",
        "\n",
        "    app_instance.app_output_widget.append_display_data(HTML(llmChatFrontend))\n",
        "else:\n",
        "    print(\"initialization failed\")"
      ],
      "metadata": {
        "id": "XaytpftQxktk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stopping The Application\n",
        "\n",
        "To gracefully stop the application and release resources, uncomment and run the code cell below."
      ],
      "metadata": {
        "id": "2Tyr6jVbxyB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# app_instance.stop_application()"
      ],
      "metadata": {
        "id": "p7-OR4umx3gL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}